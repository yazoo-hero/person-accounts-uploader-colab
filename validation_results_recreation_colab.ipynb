{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Results Recreation (Google Colab Version)\n",
    "\n",
    "Creates a Validation Result table from Workday and Calabrio data, adapted for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Google Colab Setup and Configuration ---\n",
    "\n",
    "# Set this to True if you are running in Google Colab\n",
    "IS_COLAB_ENV = True # Set to False if running locally\n",
    "\n",
    "# Base directory for your project in Google Drive or Colab environment\n",
    "# IMPORTANT: You need to adjust this path based on where you upload your project.\n",
    "# If using Google Drive, uncomment the lines below and adjust 'your_project_folder':\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Example: If your 'person-accounts-uploader-colab' folder is directly in MyDrive\n",
    "PROJECT_BASE_DIR = '/content/drive/MyDrive/person-accounts-uploader-colab'\n",
    "# If you upload the entire 'person-accounts-uploader-colab' folder to /content/ (less common for persistent storage)\n",
    "# PROJECT_BASE_DIR = '/content/person-accounts-uploader-colab'\n",
    "\n",
    "# Relative paths from PROJECT_BASE_DIR\n",
    "WORKDAY_DATA_RELATIVE_PATH = 'data/workday'\n",
    "CALABRIO_DATA_RELATIVE_PATH = 'data/calabrio'\n",
    "CONFIG_RELATIVE_PATH = 'config'\n",
    "NOTEBOOKS_MODULES_RELATIVE_PATH = 'notebooks_modules'\n",
    "\n",
    "# Install necessary libraries for Colab\n",
    "if IS_COLAB_ENV:\n",
    "    !pip install pandas openpyxl dash jupyter-dash dash-bootstrap-components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "import warnings\n",
    "from jupyter_dash import JupyterDash # Import JupyterDash for Colab\n",
    "\n",
    "# Ignore openpyxl UserWarning about default style\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# Add module path using the configured PROJECT_BASE_DIR\n",
    "# This ensures that modules in notebooks_modules can be imported\n",
    "sys.path.insert(0, str(Path(PROJECT_BASE_DIR, NOTEBOOKS_MODULES_RELATIVE_PATH)))\n",
    "\n",
    "# Set data file paths using the configured base directory\n",
    "WORKDAY_DIR = Path(PROJECT_BASE_DIR, WORKDAY_DATA_RELATIVE_PATH)\n",
    "CALABRIO_DIR = Path(PROJECT_BASE_DIR, CALABRIO_DATA_RELATIVE_PATH)\n",
    "CONFIG_DIR = Path(PROJECT_BASE_DIR, CONFIG_RELATIVE_PATH)\n",
    "\n",
    "# Update specific paths used in validation_config\n",
    "PERSON_ACCOUNTS_DIR = Path(WORKDAY_DIR, 'person_accounts')\n",
    "PEOPLE_DIR = Path(WORKDAY_DIR, 'people')\n",
    "USED_ENTRIES_DIR = Path(WORKDAY_DIR, 'used_entries')\n",
    "ACCOUNT_DATA_PATH = Path(CALABRIO_DIR, 'account_data.json')\n",
    "PERSON_DATA_PATH = Path(CALABRIO_DIR, 'person_data.json')\n",
    "CONFIG_DATA_PATH = Path(CALABRIO_DIR, 'config_data.json')\n",
    "\n",
    "# Import created modules (now that sys.path is updated)\n",
    "from validation_utils import create_filter_options, safe_get_column\n",
    "from validation_layout import (\n",
    "    create_filter_panel, create_validation_grid,\n",
    "    create_upload_grid, create_app_layout\n",
    ")\n",
    "from validation_callbacks import register_callbacks\n",
    "from validation_api import register_api_callbacks\n",
    "from validation_calculator import BalanceCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file paths are now set in the configuration cell above.\n",
    "# The following lines are commented out as they are redundant.\n",
    "# person_accounts_path = Path(PERSON_ACCOUNTS_DIR)\n",
    "# people_path = Path(PEOPLE_DIR)\n",
    "# used_entries_path = Path(USED_ENTRIES_DIR)\n",
    "# config_path = Path(CONFIG_DATA_PATH)\n",
    "# calabrio_path = Path(ACCOUNT_DATA_PATH)\n",
    "# person_path = Path(PERSON_DATA_PATH)\n",
    "# balance_rules_path = Path(CONFIG_DIR, 'balance_rules.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "### 1.1 Loading Workday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_workday_data():\n",
    "    \"\"\"Load all Workday data files.\"\"\"\n",
    "    workday_df = pd.DataFrame()\n",
    "    people_df = pd.DataFrame()\n",
    "    used_entries_df = pd.DataFrame()\n",
    "    \n",
    "    # Load person_accounts data\n",
    "    # person_accounts_path = Path('../data/workday/person_accounts')\n",
    "    if PERSON_ACCOUNTS_DIR.exists():\n",
    "        excel_files = list(PERSON_ACCOUNTS_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            workday_df = pd.read_excel(latest_file, skiprows=6, engine='openpyxl')\n",
    "            if 'WiserId' in workday_df.columns:\n",
    "                workday_df['WiserId'] = workday_df['WiserId'].astype(str)\n",
    "    \n",
    "    # Load people data\n",
    "    # people_path = Path('../data/workday/people')\n",
    "    if PEOPLE_DIR.exists():\n",
    "        excel_files = list(PEOPLE_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            people_df = pd.read_excel(latest_file, skiprows=2, engine='openpyxl')\n",
    "            if 'Latest Headcount Wiser ID' in people_df.columns:\n",
    "                people_df.rename(columns={'Latest Headcount Wiser ID': 'WiserId'}, inplace=True)\n",
    "                people_df['WiserId'] = people_df['WiserId'].astype(str)\n",
    "            if 'Latest Headcount Hire Date' in people_df.columns:\n",
    "                people_df['Latest Headcount Hire Date'] = pd.to_datetime(people_df['Latest Headcount Hire Date'])\n",
    "    \n",
    "    # Load used entries data\n",
    "    # used_entries_path = Path('../data/workday/used_entries')\n",
    "    if USED_ENTRIES_DIR.exists():\n",
    "        excel_files = list(USED_ENTRIES_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            used_entries_df = pd.read_excel(latest_file, skiprows=6, engine='openpyxl')\n",
    "            if 'WiserId' in used_entries_df.columns:\n",
    "                used_entries_df['WiserId'] = used_entries_df['WiserId'].astype(str)\n",
    "    \n",
    "    return workday_df, people_df, used_entries_df\n",
    "\n",
    "workday_df, people_df, used_entries_df = load_workday_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading Calabrio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_calabrio_data():\n",
    "    \"\"\"Load all Calabrio data files.\"\"\"\n",
    "    calabrio_df = pd.DataFrame()\n",
    "    person_df = pd.DataFrame()\n",
    "    \n",
    "    # Load account data\n",
    "    # calabrio_path = Path(ACCOUNT_DATA_PATH, \"account_data.json\")\n",
    "    if ACCOUNT_DATA_PATH.exists():\n",
    "        with open(ACCOUNT_DATA_PATH, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            calabrio_df = pd.DataFrame(json_data)\n",
    "            if 'EmploymentNumber' in calabrio_df.columns:\n",
    "                calabrio_df['EmploymentNumber'] = calabrio_df['EmploymentNumber'].astype(str)\n",
    "            if 'Accrued' in calabrio_df.columns:\n",
    "                calabrio_df['Accrued'] = pd.to_numeric(calabrio_df['Accrued'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Load person data\n",
    "    # person_path = Path(PERSON_DATA_PATH, \"person_data.json\")\n",
    "    if PERSON_DATA_PATH.exists():\n",
    "        with open(PERSON_DATA_PATH, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            person_df = pd.DataFrame(json_data)\n",
    "            if 'EmploymentNumber' in person_df.columns:\n",
    "                person_df['EmploymentNumber'] = person_df['EmploymentNumber'].astype(str)\n",
    "            person_df['EmploymentStartDate'] = pd.to_datetime(person_df['EmploymentStartDate'])\n",
    "    \n",
    "    return calabrio_df, person_df\n",
    "\n",
    "calabrio_df, calabrio_person_df = load_calabrio_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Workday Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Workday data with people data\n",
    "if not workday_df.empty and not people_df.empty:\n",
    "    if ('WiserId' in workday_df.columns and 'WiserId' in people_df.columns and\n",
    "        'Latest Headcount Hire Date' in people_df.columns and \n",
    "        'Latest Headcount Primary Work Email' in people_df.columns):\n",
    "        workday_df = pd.merge(\n",
    "            workday_df,\n",
    "            people_df[['WiserId', 'Latest Headcount Hire Date', 'Latest Headcount Primary Work Email']],\n",
    "            on='WiserId',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "# Add MappedEmploymentNumber (using WiserId as fallback)\n",
    "workday_df[\"MappedEmploymentNumber\"] = workday_df[\"WiserId\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calabrio Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge person data with calabrio_df\n",
    "if not calabrio_df.empty and not calabrio_person_df.empty:\n",
    "    if 'EmploymentNumber' in calabrio_df.columns and 'EmploymentNumber' in calabrio_person_df.columns:\n",
    "        calabrio_df = pd.merge(\n",
    "            calabrio_df,\n",
    "            calabrio_person_df[['EmploymentNumber', 'PersonId', 'BusinessUnitName', 'EmploymentStartDate']],\n",
    "            on='EmploymentNumber',\n",
    "            how='left',\n",
    "            suffixes=('', '_person')\n",
    "        )\n",
    "\n",
    "# Load config data for absence mapping\n",
    "# config_path = Path('data/calabrio/config_data.json')\n",
    "if CONFIG_DATA_PATH.exists():\n",
    "    with open(CONFIG_DATA_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the Validation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping results: 22323/22323 (100.0%) records mapped with AbsenceId\n",
      "DataFrame with AbsenceId added:\n",
      "  BusinessUnitName                          AbsenceName  \\\n",
      "0               CS   EST - Study Leave with Average Pay   \n",
      "1               CS             EST - Unpaid Study Leave   \n",
      "2               CS  Global - Compassionate Leave (Days)   \n",
      "3               CS                      Global - Me Day   \n",
      "4               CS               Global - Volunteer Day   \n",
      "\n",
      "                              AbsenceId  \n",
      "0  253e965b-1182-45a8-adaf-b03100eccc22  \n",
      "1  f783102b-0c9d-425e-97c8-b03100eccc22  \n",
    "2  008e44c8-d1cc-48c1-810f-b03100eccc22  \n",
    "3  ef2b69b6-b9e5-44cc-afe9-b03400de9a6f  \n",
    "4  674eb417-2204-41cc-9106-b03100eccc22  \n"
     ]
    }
   ],
   "source": [
    "# Function to map AbsenceId\n",
    "def map_absence_id(row, config_data):\n",
    "    \"\"\"\n",
    "    Retrieves AbsenceId from config_data using BusinessUnitName and AbsenceName.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        config_data: Configuration data dictionary\n",
    "    \n",
    "    Returns:\n",
    "        str: AbsenceId, or None if not found\n",
    "    \"\"\"\n",
    "    business_unit = row['BusinessUnitName']\n",
    "    absence_name = row['AbsenceName']\n",
    "    \n",
    "    # Check if BusinessUnitName exists in config data\n",
    "    if business_unit not in config_data:\n",
    "        # If not found, standardize using mapping function\n",
    "        from notebooks_modules.validation_utils import map_business_unit\n",
    "        mapped_bu = map_business_unit(business_unit)\n",
    "        if mapped_bu is None or mapped_bu not in config_data:\n",
    "            return None\n",
    "        business_unit = mapped_bu\n",
    "    \n",
    "    # Get absences list\n",
    "    absences = config_data.get(business_unit, {}).get('absences', {}).get('Result', [])\n",
    "    \n",
    "    # Attempt exact match\n",
    "    for absence in absences:\n",
    "        if absence.get('Name') == absence_name:\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Attempt case-insensitive match\n",
    "    absence_name_lower = absence_name.lower()\n",
    "    for absence in absences:\n",
    "        if absence.get('Name', '').lower() == absence_name_lower:\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Attempt partial match\n",
    "    for absence in absences:\n",
    "        if absence_name_lower in absence.get('Name', '').lower():\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Retry by adding 'Global -' prefix\n",
    "    if not absence_name.lower().startswith('global -'):\n",
    "        global_absence = f\"Global - {absence_name}\"\n",
    "        for absence in absences:\n",
    "            if absence.get('Name') == global_absence:\n",
    "                return absence.get('Id')\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add AbsenceId column to calabrio_df\n",
    "def add_absence_id_to_df(df, config_data):\n",
    "    \"\"\"\n",
    "    Adds an AbsenceId column to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        config_data: Configuration data dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with AbsenceId column added\n",
    "    \"\"\"\n",
    "    # List to store results\n",
    "    absence_ids = []\n",
    "    \n",
    "    # Process each row\n",
    "    for _, row in df.iterrows():\n",
    "        absence_id = map_absence_id(row, config_data)\n",
    "        absence_ids.append(absence_id)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result_df = df.copy()\n",
    "    result_df['AbsenceId'] = absence_ids\n",
    "    \n",
    "    # Mapping results statistics\n",
    "    mapped_count = sum(1 for aid in absence_ids if aid is not None)\n",
    "    total_count = len(absence_ids)\n",
    "    print(f\"Mapping results: {mapped_count}/{total_count} ({mapped_count/total_count*100:.1f}%) records mapped with AbsenceId\")\n",
    "\n",
    "    \n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# Add AbsenceId\n",
    "\n",
    "calabrio_df_with_id = add_absence_id_to_df(calabrio_df, config_data)\n",
    "\n",
    "\n",
    "\n",
    "# Verify results\n",
    "\n",
    "print(f\"DataFrame with AbsenceId added:\")\n",
    "\n",
    "print(calabrio_df_with_id[['BusinessUnitName', 'AbsenceName', 'AbsenceId']].head())\n",
    "\n",
    "\n",
    "\n",
    "# Check examples of unmapped AbsenceIds\n",
    "\n",
    "missing_ids = calabrio_df_with_id[calabrio_df_with_id['AbsenceId'].isna()]\n",
    "\n",
    "if not missing_ids.empty:\n",
    "\n",
    "    print(f\"\\nExamples of unmapped AbsenceIds ({len(missing_ids)} items):\")\n",
    "\n",
    "    print(missing_ids[['BusinessUnitName', 'AbsenceName']].head(10))\n",
    "\n",
    "    \n",
    "\n",
    "    # Aggregate combinations of BusinessUnitName and AbsenceName\n",
    "\n",
    "    missing_combinations = missing_ids.groupby(['BusinessUnitName', 'AbsenceName']).size().reset_index(name='count')\n",
    "\n",
    "    missing_combinations = missing_combinations.sort_values('count', ascending=False)\n",
    "\n",
    "    print(\"\\nMost frequent unmapped combinations:\")\n",
    "\n",
    "    print(missing_combinations.head(10))\n",
    "def create_validation_table(workday_df, calabrio_df):\n",
    "\n",
    "    \"\"\"Create validation table comparing Workday and Calabrio data.\"\"\"\n",
    "\n",
    "    # Clean and convert columns\n",
    "\n",
    "    workday_df = workday_df.copy()\n",
    "\n",
    "    calabrio_df = calabrio_df.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    workday_df[\"WiserId\"] = workday_df[\"WiserId\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    workday_df[\"Original_AbsenceType_Case\"] = workday_df[\"AbsenceType\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    workday_df[\"AbsenceType\"] = workday_df[\"AbsenceType\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    \n",
    "\n",
    "    calabrio_df[\"EmploymentNumber\"] = calabrio_df[\"EmploymentNumber\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    calabrio_df[\"AbsenceName\"] = calabrio_df[\"AbsenceName\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "\n",
    "    \n",
    "\n",
    "    # Group Calabrio data\n",
    "\n",
    "    calabrio_grouped = calabrio_df.groupby([\"EmploymentNumber\", \"AbsenceName\"]).first().reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Merge data\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "\n",
    "        workday_df,\n",
    "\n",
    "        calabrio_grouped,\n",
    "\n",
    "        left_on=[\"MappedEmploymentNumber\", \"AbsenceType\"],\n",
    "\n",
    "        right_on=[\"EmploymentNumber\", \"AbsenceName\"],\n",
    "\n",
    "        how=\"left\"\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Create display DataFrame\n",
    "\n",
    "    display_df = pd.DataFrame({\n",
    "\n",
    "        \"Workday Person Number\": merged_df[\"WiserId\"].fillna(merged_df[\"EmploymentNumber\"]),\n",
    "\n",
    "        \"Calabrio Person Number\": merged_df[\"EmploymentNumber\"],\n",
    "\n",
    "        \"Workday Absence Type\": merged_df[\"Original_AbsenceType_Case\"],\n",
    "\n",
    "        \"Calabrio Absence Type\": merged_df[\"AbsenceName\"],\n",
    "\n",
    "        \"Absence ID\": merged_df[\"AbsenceId\"],\n",
    "\n",
    "        \"Calabrio BusinessUnitName\": merged_df[\"BusinessUnitName\"],\n",
    "\n",
    "        \"StartDate\": merged_df[\"StartDate\"],\n",
    "\n",
    "        \"ContractName\": merged_df[\"ContractName\"],\n",
    "\n",
    "        \"Calabrio Balance In\": pd.to_numeric(merged_df[\"BalanceIn\"], errors=\"coerce\").fillna(0).round().astype(\"Int64\"),\n",
    "\n",
    "        \"Calabrio_Accrued\": pd.to_numeric(merged_df[\"Accrued\"].fillna(0), errors=\"coerce\").fillna(0).round().astype(\"Int64\"),\n",
    "\n",
    "        \"Calabrio Extra\": pd.to_numeric(merged_df[\"Extra\"].fillna(0), errors=\"coerce\").fillna(0).round().astype(\"Int64\"),\n",
    "\n",
    "        \"Units Approved\": pd.to_numeric(safe_get_column(merged_df, [\"Units Approved\"], 0), errors=\"coerce\").fillna(0).round().astype(\"Int64\"),\n",
    "\n",
    "        \"TrackedBy\": merged_df[\"TrackedBy\"],\n",
    "\n",
    "        \"Calabrio PersonId\": merged_df[\"PersonId\"],\n",
    "\n",
    "        \"Beginning Year Balance\": pd.to_numeric(safe_get_column(merged_df, [\"Beginning Year Balance\"], 0), errors=\"coerce\").fillna(0),\n",
    "\n",
    "        \"Accrued this year\": pd.to_numeric(safe_get_column(merged_df, [\"Accrued this year\"], 0), errors=\"coerce\").fillna(0),\n",
    "\n",
    "        \"Latest Headcount Primary Work Email\": merged_df[\"Latest Headcount Primary Work Email\"],\n",
    "\n",
    "        \"EmploymentStartDate\": merged_df[\"EmploymentStartDate\"],\n",
    "\n",
    "        \"Latest Headcount Hire Date\": merged_df[\"Latest Headcount Hire Date\"]\n",
    "\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate correct values\n",
    "\n",
    "    # balance_rules_path is now defined in the configuration cell\n",
    "\n",
    "    calculator = BalanceCalculator(Path(CONFIG_DIR, 'balance_rules.json'))\n",
    "\n",
    "    display_df[\"Correct Balance In\"], display_df[\"Correct_Accrued\"] = zip(\n",
    "\n",
    "        *display_df.apply(lambda row: calculator.calculate_correct_values(row, row[\"Workday Absence Type\"]), axis=1)\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate matches\n",
    "\n",
    "    display_df[\"Balance Match\"] = display_df.apply(\n",
    "\n",
    "        lambda row: \"✅\" if row[\"Correct Balance In\"] == row[\"Calabrio Balance In\"] else \"❌\", axis=1\n",
    "\n",
    "    )\n",
    "\n",
    "    display_df[\"Accrual Match\"] = display_df.apply(\n",
    "\n",
    "        lambda row: \"✅\" if row[\"Correct_Accrued\"] == row[\"Calabrio_Accrued\"] else \"❌\", axis=1\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate balance difference\n",
    "\n",
    "    display_df[\"Balance Difference\"] = display_df[\"Correct Balance In\"] - display_df[\"Calabrio Balance In\"]\n",
    "\n",
    "    \n",
    "\n",
    "    return display_df\n",
    "\n",
    "\n",
    "\n",
    "validation_df = create_validation_table(workday_df, calabrio_df_with_id)\n",
    "## 4. Creating the Dash Application\n",
    "# Initialize Dash application using JupyterDash for Colab compatibility\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "\n",
    "\n",
    "# Create filter options\n",
    "\n",
    "filter_options = create_filter_options(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "# Create components\n",
    "\n",
    "filter_panel = create_filter_panel(filter_options)\n",
    "\n",
    "validation_grid = create_validation_grid(validation_df)\n",
    "\n",
    "upload_grid = create_upload_grid()\n",
    "\n",
    "\n",
    "\n",
    "# Set layout\n",
    "\n",
    "app.layout = create_app_layout(filter_panel, validation_grid, upload_grid)\n",
    "\n",
    "\n",
    "\n",
    "# Register callbacks\n",
    "\n",
    "register_callbacks(app, validation_df)\n",
    "\n",
    "register_api_callbacks(app)\n",
    "# Activate\n",
    "# Run the Dash application in inline mode for Google Colab\n",
    "\n",
    "app.run_server(mode='inline', debug=True, port=8050)\n"
   ]
  }
 ]
}
