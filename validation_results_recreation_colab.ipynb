{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Results Recreation (Google Colab Version)\n",
    "\n",
    "Creates a Validation Result table from Workday and Calabrio data, adapted for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Google Colab Setup and Configuration ---\n",
    "\n",
    "# Set this to True if you are running in Google Colab\n",
    "IS_COLAB_ENV = True # Set to False if running locally\n",
    "\n",
    "# Base directory for your project in Google Drive or Colab environment\n",
    "# IMPORTANT: You need to adjust this path based on where you upload your project.\n",
    "# If using Google Drive, uncomment the lines below and adjust 'your_project_folder':\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PROJECT_BASE_DIR = '/content/drive/MyDrive/your_project_folder/person_accounts_migration_tool'\n",
    "\n",
    "# Example if you upload the entire 'person_accounts_migration_tool' folder to /content/\n",
    "PROJECT_BASE_DIR = '.'\n",
    "\n",
    "# Relative paths from PROJECT_BASE_DIR\n",
    "WORKDAY_DATA_RELATIVE_PATH = 'data/workday'\n",
    "CALABRIO_DATA_RELATIVE_PATH = 'data/calabrio'\n",
    "CONFIG_RELATIVE_PATH = 'config'\n",
    "NOTEBOOKS_MODULES_RELATIVE_PATH = 'notebooks_modules'\n",
    "\n",
    "# Install necessary libraries for Colab\n",
    "if IS_COLAB_ENV:\n",
    "    !pip install pandas openpyxl dash jupyter-dash dash-bootstrap-components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import dash\n",
    "from dash import html, dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "import warnings\n",
    "from jupyter_dash import JupyterDash # Import JupyterDash for Colab\n",
    "\n",
    "# Ignore openpyxl UserWarning about default style\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# Add module path using the configured PROJECT_BASE_DIR\n",
    "# This ensures that modules in notebooks_modules can be imported\n",
    "sys.path.insert(0, str(Path(PROJECT_BASE_DIR, NOTEBOOKS_MODULES_RELATIVE_PATH).parent))\n",
    "\n",
    "# Set data file paths using the configured base directory\n",
    "WORKDAY_DIR = Path(PROJECT_BASE_DIR, WORKDAY_DATA_RELATIVE_PATH)\n",
    "CALABRIO_DIR = Path(PROJECT_BASE_DIR, CALABRIO_DATA_RELATIVE_PATH)\n",
    "CONFIG_DIR = Path(PROJECT_BASE_DIR, CONFIG_RELATIVE_PATH)\n",
    "\n",
    "# Update specific paths used in validation_config\n",
    "PERSON_ACCOUNTS_DIR = Path(WORKDAY_DIR, 'person_accounts')\n",
    "PEOPLE_DIR = Path(WORKDAY_DIR, 'people')\n",
    "USED_ENTRIES_DIR = Path(WORKDAY_DIR, 'used_entries')\n",
    "ACCOUNT_DATA_PATH = Path(CALABRIO_DIR, 'account_data.json')\n",
    "PERSON_DATA_PATH = Path(CALABRIO_DIR, 'person_data.json')\n",
    "CONFIG_DATA_PATH = Path(CALABRIO_DIR, 'config_data.json')\n",
    "\n",
    "# Import created modules (now that sys.path is updated)\n",
    "from notebooks_modules.validation_config import (\n",
    "    WORKDAY_DIR, PERSON_ACCOUNTS_DIR,\n",
    "    PEOPLE_DIR, USED_ENTRIES_DIR, CALABRIO_DIR,CONFIG_DIR,\n",
    "    ACCOUNT_DATA_PATH, PERSON_DATA_PATH, CONFIG_DATA_PATH\n",
    ")\n",
    "from notebooks_modules.validation_utils import create_filter_options, safe_get_column\n",
    "from notebooks_modules.validation_layout import (\n",
    "    create_filter_panel, create_validation_grid,\n",
    "    create_upload_grid, create_app_layout\n",
    ")\n",
    "from notebooks_modules.validation_callbacks import register_callbacks\n",
    "from notebooks_modules.validation_api import register_api_callbacks\n",
    "from notebooks_modules.validation_calculator import BalanceCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file paths are now set in the configuration cell above.\n",
    "# The following lines are commented out as they are redundant.\n",
    "# person_accounts_path = Path(PERSON_ACCOUNTS_DIR)\n",
    "# people_path = Path(PEOPLE_DIR)\n",
    "# used_entries_path = Path(USED_ENTRIES_DIR)\n",
    "# config_path = Path(CONFIG_DATA_PATH)\n",
    "# calabrio_path = Path(ACCOUNT_DATA_PATH)\n",
    "# person_path = Path(PERSON_DATA_PATH)\n",
    "# balance_rules_path = Path(CONFIG_DIR, 'balance_rules.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "### 1.1 Loading Workday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_workday_data():\n",
    "    \"\"\"Load all Workday data files.\"\"\"\n",
    "    workday_df = pd.DataFrame()\n",
    "    people_df = pd.DataFrame()\n",
    "    used_entries_df = pd.DataFrame()\n",
    "    \n",
    "    # Load person_accounts data\n",
    "    # person_accounts_path = Path('../data/workday/person_accounts')\n",
    "    if PERSON_ACCOUNTS_DIR.exists():\n",
    "        excel_files = list(PERSON_ACCOUNTS_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            workday_df = pd.read_excel(latest_file, skiprows=6, engine='openpyxl')\n",
    "            if 'WiserId' in workday_df.columns:\n",
    "                workday_df['WiserId'] = workday_df['WiserId'].astype(str)\n",
    "    \n",
    "    # Load people data\n",
    "    # people_path = Path('../data/workday/people')\n",
    "    if PEOPLE_DIR.exists():\n",
    "        excel_files = list(PEOPLE_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            people_df = pd.read_excel(latest_file, skiprows=2, engine='openpyxl')\n",
    "            if 'Latest Headcount Wiser ID' in people_df.columns:\n",
    "                people_df.rename(columns={'Latest Headcount Wiser ID': 'WiserId'}, inplace=True)\n",
    "                people_df['WiserId'] = people_df['WiserId'].astype(str)\n",
    "            if 'Latest Headcount Hire Date' in people_df.columns:\n",
    "                people_df['Latest Headcount Hire Date'] = pd.to_datetime(people_df['Latest Headcount Hire Date'])\n",
    "    \n",
    "    # Load used entries data\n",
    "    # used_entries_path = Path('../data/workday/used_entries')\n",
    "    if USED_ENTRIES_DIR.exists():\n",
    "        excel_files = list(USED_ENTRIES_DIR.glob(\"*.xlsx\"))\n",
    "        if excel_files:\n",
    "            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "            used_entries_df = pd.read_excel(latest_file, skiprows=6, engine='openpyxl')\n",
    "            if 'WiserId' in used_entries_df.columns:\n",
    "                used_entries_df['WiserId'] = used_entries_df['WiserId'].astype(str)\n",
    "    \n",
    "    return workday_df, people_df, used_entries_df\n",
    "\n",
    "workday_df, people_df, used_entries_df = load_workday_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading Calabrio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_calabrio_data():\n",
    "    \"\"\"Load all Calabrio data files.\"\"\"\n",
    "    calabrio_df = pd.DataFrame()\n",
    "    person_df = pd.DataFrame()\n",
    "    \n",
    "    # Load account data\n",
    "    # calabrio_path = Path(ACCOUNT_DATA_PATH, \"account_data.json\")\n",
    "    if ACCOUNT_DATA_PATH.exists():\n",
    "        with open(ACCOUNT_DATA_PATH, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            calabrio_df = pd.DataFrame(json_data)\n",
    "            if 'EmploymentNumber' in calabrio_df.columns:\n",
    "                calabrio_df['EmploymentNumber'] = calabrio_df['EmploymentNumber'].astype(str)\n",
    "            if 'Accrued' in calabrio_df.columns:\n",
    "                calabrio_df['Accrued'] = pd.to_numeric(calabrio_df['Accrued'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Load person data\n",
    "    # person_path = Path(PERSON_DATA_PATH, \"person_data.json\")\n",
    "    if PERSON_DATA_PATH.exists():\n",
    "        with open(PERSON_DATA_PATH, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            person_df = pd.DataFrame(json_data)\n",
    "            if 'EmploymentNumber' in person_df.columns:\n",
    "                person_df['EmploymentNumber'] = person_df['EmploymentNumber'].astype(str)\n",
    "            person_df['EmploymentStartDate'] = pd.to_datetime(person_df['EmploymentStartDate'])\n",
    "    \n",
    "    return calabrio_df, person_df\n",
    "\n",
    "calabrio_df, calabrio_person_df = load_calabrio_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Workday Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Workday data with people data\n",
    "if not workday_df.empty and not people_df.empty:\n",
    "    if ('WiserId' in workday_df.columns and 'WiserId' in people_df.columns and\n",
    "        'Latest Headcount Hire Date' in people_df.columns and \n",
    "        'Latest Headcount Primary Work Email' in people_df.columns):\n",
    "        workday_df = pd.merge(\n",
    "            workday_df,\n",
    "            people_df[['WiserId', 'Latest Headcount Hire Date', 'Latest Headcount Primary Work Email']],\n",
    "            on='WiserId',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "# Add MappedEmploymentNumber (using WiserId as fallback)\n",
    "workday_df[\"MappedEmploymentNumber\"] = workday_df[\"WiserId\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calabrio Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge person data with calabrio_df\n",
    "if not calabrio_df.empty and not calabrio_person_df.empty:\n",
    "    if 'EmploymentNumber' in calabrio_df.columns and 'EmploymentNumber' in calabrio_person_df.columns:\n",
    "        calabrio_df = pd.merge(\n",
    "            calabrio_df,\n",
    "            calabrio_person_df[['EmploymentNumber', 'PersonId', 'BusinessUnitName', 'EmploymentStartDate']],\n",
    "            on='EmploymentNumber',\n",
    "            how='left',\n",
    "            suffixes=('', '_person')\n",
    "        )\n",
    "\n",
    "# Load config data for absence mapping\n",
    "# config_path = Path('data/calabrio/config_data.json')\n",
    "if CONFIG_DATA_PATH.exists():\n",
    "    with open(CONFIG_DATA_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the Validation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping results: 22323/22323 (100.0%) records mapped with AbsenceId\n",
      "DataFrame with AbsenceId added:\n",
      "  BusinessUnitName                          AbsenceName  \\\n",
      "0               CS   EST - Study Leave with Average Pay   \n",
      "1               CS             EST - Unpaid Study Leave   \n",
      "2               CS  Global - Compassionate Leave (Days)   \n",
      "3               CS                      Global - Me Day   \n",
      "4               CS               Global - Volunteer Day   \n",
      "\n",
      "                              AbsenceId  \n",
      "0  253e965b-1182-45a8-adaf-b03100eccc22  \n",
      "1  f783102b-0c9d-425e-97c8-b03100eccc22  \n",
      "2  008e44c8-d1cc-48c1-810f-b03100eccc22  \n",
      "3  ef2b69b6-b9e5-44cc-afe9-b03400de9a6f  \n",
      "4  674eb417-2204-41cc-9106-b03100eccc22  \n"
     ]
    }
   ],
   "source": [
    "# Function to map AbsenceId\n",
    "def map_absence_id(row, config_data):\n",
    "    \"\"\"\n",
    "    Retrieves AbsenceId from config_data using BusinessUnitName and AbsenceName.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row\n",
    "        config_data: Configuration data dictionary\n",
    "    \n",
    "    Returns:\n",
    "        str: AbsenceId, or None if not found\n",
    "    \"\"\"\n",
    "    business_unit = row['BusinessUnitName']\n",
    "    absence_name = row['AbsenceName']\n",
    "    \n",
    "    # Check if BusinessUnitName exists in config data\n",
    "    if business_unit not in config_data:\n",
    "        # If not found, standardize using mapping function\n",
    "        from notebooks_modules.validation_utils import map_business_unit\n",
    "        mapped_bu = map_business_unit(business_unit)\n",
    "        if mapped_bu is None or mapped_bu not in config_data:\n",
    "            return None\n",
    "        business_unit = mapped_bu\n",
    "    \n",
    "    # Get absences list\n",
    "    absences = config_data.get(business_unit, {}).get('absences', {}).get('Result', [])\n",
    "    \n",
    "    # Attempt exact match\n",
    "    for absence in absences:\n",
    "        if absence.get('Name') == absence_name:\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Attempt case-insensitive match\n",
    "    absence_name_lower = absence_name.lower()\n",
    "    for absence in absences:\n",
    "        if absence.get('Name', '').lower() == absence_name_lower:\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Attempt partial match\n",
    "    for absence in absences:\n",
    "        if absence_name_lower in absence.get('Name', '').lower():\n",
    "            return absence.get('Id')\n",
    "    \n",
    "    # Retry by adding 'Global -' prefix\n",
    "    if not absence_name.lower().startswith('global -'):\n",
    "        global_absence = f\"Global - {absence_name}\"\n",
    "        for absence in absences:\n",
    "            if absence.get('Name') == global_absence:\n",
    "                return absence.get('Id')\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add AbsenceId column to calabrio_df\n",
    "def add_absence_id_to_df(df, config_data):\n",
    "    \"\"\"\n",
    "    Adds an AbsenceId column to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        config_data: Configuration data dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with AbsenceId column added\n",
    "    \"\"\"\n",
    "    # List to store results\n",
    "    absence_ids = []\n",
    "    \n",
    "    # Process each row\n",
    "    for _, row in df.iterrows():\n",
    "        absence_id = map_absence_id(row, config_data)\n",
    "        absence_ids.append(absence_id)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result_df = df.copy()\n",
    "    result_df['AbsenceId'] = absence_ids\n",
    "    \n",
    "    # Mapping results statistics\n",
    "    mapped_count = sum(1 for aid in absence_ids if aid is not None)\n",
    "    total_count = len(absence_ids)\n",
    "    print(f"Mapping results: {mapped_count}/{total_count} ({mapped_count/total_count*100:.1f}%) records mapped with AbsenceId")

    

    return result_df



# Add AbsenceId

calabrio_df_with_id = add_absence_id_to_df(calabrio_df, config_data)



# Verify results

print(f"DataFrame with AbsenceId added:")

print(calabrio_df_with_id[['BusinessUnitName', 'AbsenceName', 'AbsenceId']].head())



# Check examples of unmapped AbsenceIds

missing_ids = calabrio_df_with_id[calabrio_df_with_id['AbsenceId'].isna()]

if not missing_ids.empty:

    print(f"\nExamples of unmapped AbsenceIds ({len(missing_ids)} items):")

    print(missing_ids[['BusinessUnitName', 'AbsenceName']].head(10))

    

    # Aggregate combinations of BusinessUnitName and AbsenceName

    missing_combinations = missing_ids.groupby(['BusinessUnitName', 'AbsenceName']).size().reset_index(name='count')

    missing_combinations = missing_combinations.sort_values('count', ascending=False)

    print("\nMost frequent unmapped combinations:")

    print(missing_combinations.head(10))
def create_validation_table(workday_df, calabrio_df):

    """Create validation table comparing Workday and Calabrio data."""

    # Clean and convert columns

    workday_df = workday_df.copy()

    calabrio_df = calabrio_df.copy()

    

    workday_df["WiserId"] = workday_df["WiserId"].fillna("").astype(str).str.strip().str.lower()

    workday_df["Original_AbsenceType_Case"] = workday_df["AbsenceType"].fillna("").astype(str).str.strip()

    workday_df["AbsenceType"] = workday_df["AbsenceType"].fillna("").astype(str).str.strip().str.lower()

    

    calabrio_df["EmploymentNumber"] = calabrio_df["EmploymentNumber"].fillna("").astype(str).str.strip().str.lower()

    calabrio_df["AbsenceName"] = calabrio_df["AbsenceName"].fillna("").astype(str).str.strip().str.lower()

    

    # Group Calabrio data

    calabrio_grouped = calabrio_df.groupby(["EmploymentNumber", "AbsenceName"]).first().reset_index()

    

    # Merge data

    merged_df = pd.merge(

        workday_df,

        calabrio_grouped,

        left_on=["MappedEmploymentNumber", "AbsenceType"],

        right_on=["EmploymentNumber", "AbsenceName"],

        how="left"

    )

    

    # Create display DataFrame

    display_df = pd.DataFrame({

        "Workday Person Number": merged_df["WiserId"].fillna(merged_df["EmploymentNumber"]),

        "Calabrio Person Number": merged_df["EmploymentNumber"],

        "Workday Absence Type": merged_df["Original_AbsenceType_Case"],

        "Calabrio Absence Type": merged_df["AbsenceName"],

        "Absence ID": merged_df["AbsenceId"],

        "Calabrio BusinessUnitName": merged_df["BusinessUnitName"],

        "StartDate": merged_df["StartDate"],

        "ContractName": merged_df["ContractName"],

        "Calabrio Balance In": pd.to_numeric(merged_df["BalanceIn"], errors="coerce").fillna(0).round().astype("Int64"),

        "Calabrio_Accrued": pd.to_numeric(merged_df["Accrued"].fillna(0), errors="coerce").fillna(0).round().astype("Int64"),

        "Calabrio Extra": pd.to_numeric(merged_df["Extra"].fillna(0), errors="coerce").fillna(0).round().astype("Int64"),

        "Units Approved": pd.to_numeric(safe_get_column(merged_df, ["Units Approved"], 0), errors="coerce").fillna(0).round().astype("Int64"),

        "TrackedBy": merged_df["TrackedBy"],

        "Calabrio PersonId": merged_df["PersonId"],

        "Beginning Year Balance": pd.to_numeric(safe_get_column(merged_df, ["Beginning Year Balance"], 0), errors="coerce").fillna(0),

        "Accrued this year": pd.to_numeric(safe_get_column(merged_df, ["Accrued this year"], 0), errors="coerce").fillna(0),

        "Latest Headcount Primary Work Email": merged_df["Latest Headcount Primary Work Email"],

        "EmploymentStartDate": merged_df["EmploymentStartDate"],

        "Latest Headcount Hire Date": merged_df["Latest Headcount Hire Date"]

    })

    

    # Calculate correct values

    # balance_rules_path is now defined in the configuration cell

    calculator = BalanceCalculator(Path(CONFIG_DIR, 'balance_rules.json'))

    display_df["Correct Balance In"], display_df["Correct_Accrued"] = zip(

        *display_df.apply(lambda row: calculator.calculate_correct_values(row, row["Workday Absence Type"]), axis=1)

    )

    

    # Calculate matches

    display_df["Balance Match"] = display_df.apply(

        lambda row: "✅" if row["Correct Balance In"] == row["Calabrio Balance In"] else "❌", axis=1

    )

    display_df["Accrual Match"] = display_df.apply(

        lambda row: "✅" if row["Correct_Accrued"] == row["Calabrio_Accrued"] else "❌", axis=1

    )

    

    # Calculate balance difference

    display_df["Balance Difference"] = display_df["Correct Balance In"] - display_df["Calabrio Balance In"]

    

    return display_df



validation_df = create_validation_table(workday_df, calabrio_df_with_id)
## 4. Creating the Dash Application
# Initialize Dash application using JupyterDash for Colab compatibility

app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])



# Create filter options

filter_options = create_filter_options(validation_df)



# Create components

filter_panel = create_filter_panel(filter_options)

validation_grid = create_validation_grid(validation_df)

upload_grid = create_upload_grid()



# Set layout

app.layout = create_app_layout(filter_panel, validation_grid, upload_grid)



# Register callbacks

register_callbacks(app, validation_df)

register_api_callbacks(app)
# Activate
# Run the Dash application in inline mode for Google Colab

app.run_server(mode='inline', debug=True, port=8050)
</file_content>

Now that you have the latest state of the file, try the operation again with fewer, more precise SEARCH blocks. For large files especially, it may be prudent to try to limit yourself to <5 SEARCH/REPLACE blocks at a time, then wait for the user to respond with the result of the operation before following up with another replace_in_file call to make additional edits.
(If you run into this error 3 times in a row, you may use the write_to_file tool as a fallback.)
</error><environment_details>
# VSCode Visible Files
notebooks/validation_results_recreation_colab.ipynb

# VSCode Open Tabs
notebooks/validation_results_recreation.ipynb
notebooks/validation_results_recreation_colab.ipynb

# Current Time
21/05/2025, 5:12:41 pm (Europe/Tallinn, UTC+3:00)

# Context Window Usage
162,450 / 1,048.576K tokens used (15%)

# Current Mode
ACT MODE
</environment_details>
